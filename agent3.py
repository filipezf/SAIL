# -*- coding: utf-8 -*-
"""
Created on Thu Dec 12 10:51:53 2019

@author: u4ft
"""
import env
import random
import numpy as np
import random
import os 
import copy
import itertools
import base_agent as ba
import torch


pop = env.pop

def coin(p):
    return random.random() < p
    

Z = ba.Z    
cnt = 0
    
class Agent(ba.Agent):
        
          
    def __str__(self):
        return 'a_' + str(self.id)                        
      
    def step(self):
        for i in range(50):
            self.step0()
            if wm[OUT] > 1:
                break
        
    def step0(self):
        
        
        wm[:] = wm[:] + layer * gating
        
        
        a = ponderoptions(gating)
        
        # bdi -> predict weights
        
        
        act = wm[:]
        act.txt = wm[:]
        Z( s=s1, v = v1, q = q1, txt = txt1)
        
        u = base_calcU() #u = mate + energy + friend+ reciprocal + kin
        
        short_tape.append( s, a, s1, a1)
        #layer[i+1]= ReLu( layer[i]) # slow time
        
        
        store_em() # wm[i]  self.em
        wm[i:] = retrieve_em()  self.em
        
        layer[0][0][0:50]= inp.s
        layer[0][1][0:50] = s
        
        layer[0][0][50*0 : 50*1]= inp.v
        layer[0][1][50*1 : 50*2] = v
        
        layer[i] = ReLU( sum layer[j * wij])
        
        
        magnitude_neurons[]
        convolutional
        typical_p
        
        w_i_j # store pattern
        #backpropagation
        
        
        #wm_ctx/task # stripes
        
        #generate
        
        ctx -> automatic vs generic
           
        
        #uncertainty          
        #rehearsal(): # dream
        
   def addReward(r):
       pass
   
    
        